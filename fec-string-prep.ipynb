{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the Git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import timeit\n",
    "import seaborn as sns\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import matplotlib as mpl\n",
    "import time\n",
    "import csv\n",
    "import math\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "from time import sleep\n",
    "import sys\n",
    "import pickle\n",
    "import codecs\n",
    "import threading\n",
    "os.environ['PATH'] += \";C:\\\\Program Files\\\\Git\\\\bin\"\n",
    "\n",
    "'''# ---------------- Old Toolbar Updated -------------------------#\n",
    "toolbar_width = 40\n",
    "\n",
    "# setup toolbar\n",
    "sys.stdout.write(\"[%s]\" % (\" \" * toolbar_width))\n",
    "sys.stdout.flush()\n",
    "sys.stdout.write(\"\\b\" * (toolbar_width+1)) \n",
    "'''\n",
    "\n",
    "def progbar(i,j):\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.write(\"%.2f%%\" % round((i / j) * 100,2))\n",
    "    sys.stdout.flush()\n",
    "    sleep(0.001)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install C:/Users/jhb362/Dropbox/Programs/python_Levenshtein-0.12.0-cp35-none-win_amd64.whl"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "%%bash\n",
    "rm -rf fec-data\n",
    "git clone git@github.com:jbisbee1/fec-data.git"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "%%bash\n",
    "git log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opening the full data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "recipients = pd.read_stata(\"../../rawrecipients.dta\")\n",
    "recipients.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuzzy matching on firm names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use the fuzzywuzzy package and run through each firm name, comparing it to the remaining 5.6 million in the dataset. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "recclean = recipients['raw_recipient']\n",
    "recclean.head()\n",
    "len(recclean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the firm names is compared against a master list which is stripped of any non-string types (although I'm not sure why any would be in there, there seem to be a few.) I also drop duplicates but, again, I don't think there should be too many in there, given all the work already conducted using Stata."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "uniques = recclean.drop_duplicates()\n",
    "select = [isinstance(e,str) for e in uniques]\n",
    "uniques2 = uniques[select]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "select = [isinstance(e,str) for e in recclean]\n",
    "recclean2 = recclean[select]\n",
    "print(len(recclean2), len(uniques2))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "name2 = 'rawrecspickle'\n",
    "fileObject = open(name2,'wb')\n",
    "pickle.dump(uniques2,fileObject)\n",
    "fileObject.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fileObject = open('rawrecspickle','rb')\n",
    "uniquerecs = pickle.load(fileObject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4820948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'wilshire state bank',\n",
       " 'weber construction',\n",
       " 'aptos chamber of commerce',\n",
       " 'dice photography']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanedrec = pickle.load(open('cleanedrecs.p','rb'))\n",
    "cleanedrec = list(set(cleanedrec))\n",
    "print(len(cleanedrec))\n",
    "cleanedrec[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Confirmed that there are no duplicates in the list. \n",
    "\n",
    "Now we're off to the races. This is going to take a goddamned long time so smoke 'em if you got 'em. We go through every firm name in the full list of 5.6m and compare the string to all of the others using the fuzzywuzzy method \"process\". \n",
    "\n",
    "Whoa...too long. I'm going to try and run this on the cluster. No sense in gumming up my machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000 510000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "510.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictter = dict(zip(range(569),range(0,6 * 10 ** 6,10 ** 4)))\n",
    "start = dictter[50] #dictter[int(sys.argv[1])]\n",
    "end = dictter[50+1] #dictter[int(sys.argv[1]+1)]\n",
    "print(start,end)\n",
    "end = end/1000 # Comment this line out to run the 10k chunks.\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threading test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def pairs(rec,uniques):\n",
    "    return [rec,[x[0] for x in process.extractBests(rec,uniques,scorer=fuzz.token_set_ratio,score_cutoff=90)]]\n",
    "    print('worker: %s' % rec)\n",
    "\n",
    "print(cleanedrec[1])\n",
    "test = pairs(cleanedrec[1],cleanedrec)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "threads = []\n",
    "for i in range(5):\n",
    "    t = threading.Thread(target = pairs,args = (uniquerecs[i],uniquerecs[:100]))\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "print(t)\n",
    "print(threads[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to figure out how to extract content from these threads. Right now, I'm only messing around with the thread workers themselves but surely the function output is stored somewhere within them? I have also messed around with multiprocessing and the joblib packages but these don't seem to work within Jupyter (and require a cluge to work in Windows at all via command line calls)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A basic loop for pairing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before I get started, I want to do some basic text cleaning to convert things like \"corporation\", \"incorporated\", \"city\", \"university\", etc. (Note that this is raw'ed out after I successfully ran it because it takes for goddamned ever.)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import re\n",
    "\n",
    "def clning(name):\n",
    "    test = name.lower()\n",
    "    test = re.sub('(\\')','',test)\n",
    "    rx = re.compile('\\W+')\n",
    "    test = rx.sub(' ',test).strip()\n",
    "    test = re.sub('(^|\\s)inc($|\\s)','',test)\n",
    "    test = re.sub('(^|\\s)incorporated($|\\s)','',test)\n",
    "    test = re.sub('(^|\\s)assn($|\\s)','',test)\n",
    "    test = re.sub('(^|\\s)assoc($|\\s)','',test)\n",
    "    test = re.sub('(^|\\s)association($|\\s)','',test)\n",
    "    test = re.sub('(^|\\s)corp($|\\s)','',test)\n",
    "    test = re.sub('(^|\\s)corporation($|\\s)','',test)\n",
    "    test = re.sub('(^|\\s)co($|\\s)','',test)\n",
    "    test = re.sub('(^|\\s)company($|\\s)','',test)\n",
    "    test = re.sub('(^|\\s)llc($|\\s)','',test)\n",
    "    test = re.sub('(^|\\s)city($|\\s)',' c ',test)\n",
    "    test = re.sub('(^|\\s)county($|\\s)',' n ',test)\n",
    "    test = re.sub('(^|\\s)cong(ress?)($|\\s)',' cng ',test)\n",
    "    test = re.sub('(^|\\s)senat(e|or)($|\\s)',' sen ',test)\n",
    "    test = re.sub('(^|\\s)campaign($|\\s)',' cmp ',test)\n",
    "    test = re.sub('univ(ersity?)($|\\s)',' unv ',test)\n",
    "    test = test.strip()\n",
    "    return test\n",
    "\n",
    "cleaned = []\n",
    "counter = 1\n",
    "for x in uniquerecs:\n",
    "    progbar(counter,len(uniquerecs))\n",
    "    counter +=1\n",
    "    cleaned.append(clning(x))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "pickle.dump(cleaned,open(\"cleanedrecs.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "pairings=[]\n",
    "counter = 0\n",
    "\n",
    "for rec in cleaned[start:end]:\n",
    "    pairings.append([rec,[x[0] for x in process.extractBests(rec,cleaned[:1000],scorer=fuzz.token_set_ratio,score_cutoff=90)]])\n",
    "    progbar(counter,(end-start)-1)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEED TO SPEED THINGS UP. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some searching led me to https://github.com/schollz/string_matching which uses a dictionary to dramatically reduce the number of words in the full list that are relevant for the string matching algorithm. I'm going to try and hack it into my process. (Right now, the process of 9 weeks of 300 jobs running constantly seems a little insane.) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%bash\n",
    "pip install wget"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import wget"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "words = wget.download('http://www-personal.umich.edu/~jlawler/wordlist')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "%%bash\n",
    "git clone git@github.com:schollz/string_matching.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up pickle files via setup.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I need to recreate the functions since I'm not sure how to import from a folder that's locally stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateSearchableHashFromList(listOfStrings):\n",
    "    sHash = {}\n",
    "    for string in listOfStrings:\n",
    "        for i in range(0,len(string)-2):\n",
    "            doublet = string[i:i+3].lower()\n",
    "            if doublet not in sHash:\n",
    "                sHash[doublet] = []\n",
    "            sHash[doublet].append(string)\n",
    "    return sHash"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "testlist = []\n",
    "for line in cleanedrec[:1000]:\n",
    "    testlist.append(line.strip().title())\n",
    "testlist[:5]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cleanedrec[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dumping defaultList took 5.431874513626099\n",
      "dumping defaultHash took 73.74902558326721\n"
     ]
    }
   ],
   "source": [
    "import string_matching\n",
    "from time import time \n",
    "\n",
    "listchunk = cleanedrec\n",
    "\n",
    "# Make a list of words, save it as \"defaultList.p\"\n",
    "tt = time()\n",
    "defaultList = []\n",
    "for line in listchunk:\n",
    "        defaultList.append(line.strip())\n",
    "pickle.dump(defaultList,open('defaultList.p','wb'))\n",
    "print ('dumping defaultList took ' + str(time()-tt))\n",
    "\n",
    "\n",
    "# Generate the searchable hash, save it as \"defaultHash.p\"\n",
    "tt = time()\n",
    "defaultHash = generateSearchableHashFromList(defaultList)\n",
    "pickle.dump(defaultHash,open('defaultHash.p','wb'))\n",
    "print ('dumping defaultHash took ' + str(time()-tt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "defaultList = pickle.load(open('defaultList.p','rb'))\n",
    "defaultHash = pickle.load(open('defaultHash.p','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is all old but I think it will be useful to keep around. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the tutorial provided by ja boy schollz is cool and all but I think I'm going to keep my pairings function and then just run it on the subset of data associated with the best matches according to the hash approach. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def compareStrings(strings):\n",
    "    leven1 = fuzz.token_set_ratio(strings[0],strings[1])\n",
    "    leven2 = Levenshtein.ratio(str(strings[0]),str(strings[1]))\n",
    "    return (strings[0],strings[1],leven1+leven2*100,leven2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try a short example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section recreates the content of the searchThroughList function"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def searchThroughList(searchString,listOfStrings=defaultList):\n",
    "    if len(listOfStrings)==0:\n",
    "        return \"You need to save a hash to defaultHash.p if you want to load one automatically!\"\n",
    "    stringList = []\n",
    "    counter = 0\n",
    "    for string in listOfStrings:\n",
    "        progbar(counter,len(listOfStrings)-1)\n",
    "        stringList.append((searchString.lower(),string.lower()))\n",
    "        \n",
    "    pool2 = Pool(2) \n",
    "    results = pool2.map(compareStrings, stringList)\n",
    "    pool2.close()\n",
    "    pool2.join()\n",
    "    #print (sorted(results, key=operator.itemgetter(2, 3), reverse=True))[:10]\n",
    "    topResult = (sorted(results, key=operator.itemgetter(2, 3), reverse=True))[0]\n",
    "    print( \"Top 5 matches: \",)\n",
    "    for it in (sorted(results, key=operator.itemgetter(2, 3), reverse=True))[:5]:\n",
    "        print( it[1] + \"(\"+str(round(it[2],1))+\") \",)\n",
    "    print (\"\")\n",
    "    return listOfStrings[[x.lower() for x in listOfStrings].index(topResult[1])]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "stringList=[]\n",
    "counter = 0\n",
    "searchString=cleanedrec[1]\n",
    "for string in defaultList[:1000]:\n",
    "    progbar(counter,len(defaultList)-1)\n",
    "    counter += 1\n",
    "    stringList.append((searchString.lower(),string.lower()))\n",
    "stringList[1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import Levenshtein\n",
    "pairings = []\n",
    "counter=1\n",
    "for i in stringList:\n",
    "    progbar(counter,len(stringList)-1)\n",
    "    counter+=1\n",
    "    pairings.append(compareStrings(i))\n",
    "pairings[:5]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import operator\n",
    "# Top five is a good movie\n",
    "topResult = (sorted(pairings, key=operator.itemgetter(2,3),reverse=True))[0]\n",
    "for it in (sorted(pairings,key=operator.itemgetter(2,3),reverse=True))[:5]:\n",
    "      print(it[1]+\"(\"+str(round(it[2],1)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stringList[:5]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "pool2 = Pool(2)\n",
    "results = pool2.map(compareStrings,stringList)\n",
    "pool2.close()\n",
    "pool2.join()\n",
    "results[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section recreates the searchThroughHash function. This is the part I want to keep, but instead of running the searchThroughList() function at the end, I want to run my own extractBests function borrowed from fuzzywuzzy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pairs(searchString,stringList):\n",
    "    return [searchString,[x[0] for x in process.extractBests(searchString,stringList,scorer=fuzz.token_set_ratio,score_cutoff=90)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def searchThroughHash(searchString,sHash=defaultHash):\n",
    "    if len(sHash)==0:\n",
    "        return \"You need to save a hash to defaultHash.p if you want to load one automatically!\"\n",
    "    searchString = searchString.lower()\n",
    "    possibleStrings = []\n",
    "    for i in range(0,len(searchString)-2):\n",
    "        doublet = searchString[i:i+3]\n",
    "        if doublet in sHash:\n",
    "            possibleStrings += sHash[doublet]\n",
    "    c = Counter(possibleStrings)\n",
    "    mostPossible = []\n",
    "    for p in c.most_common(1000):\n",
    "        mostPossible.append(p[0])\n",
    "    return pairs(searchString,mostPossible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wilshire state bank', ['wilshire state bank', 'c state bank']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searchThroughHash(cleanedrec[1],defaultHash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.15%"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Gerald J Katherine I Kurian',\n",
       " 'Katherine Marcell Hazen Rev Living Trust Katherine Kirk Hazen T',\n",
       " 'Gerald Katherine Goss',\n",
       " 'Catherine Square Catherine Square',\n",
       " 'Katherine A Ronald J Kochevar']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searchString=cleanedrec[1].lower()\n",
    "possibleStrings=[]\n",
    "counter = 1\n",
    "for i in range(0,len(searchString)-2):\n",
    "    progbar(counter,len(searchString)-1)\n",
    "    counter+=1\n",
    "    doublet = searchString[i:i+3]\n",
    "    if doublet in defaultHash:\n",
    "        possibleStrings +=defaultHash[doublet]\n",
    "possibleStrings[:5]\n",
    "c = Counter(possibleStrings)\n",
    "mostPossible = []\n",
    "for p in c.most_common(1000):\n",
    "    mostPossible.append(p[0])\n",
    "mostPossible[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def testStringMatchin(string):\n",
    "    print (\"trying with \" + string)\n",
    "    tt = time()\n",
    "    print ('trying list look-up took ' + str(time()-tt))\n",
    "    print (\"\\n\")\n",
    "\n",
    "    print (\"searching through entire list...\")\n",
    "    tt = time()\n",
    "    result = searchThroughList(string)\n",
    "    print (\"Best match: \" + result)\n",
    "    print ('searching through entire list took ' + str(time()-tt))\n",
    "    print (\"\\n\")\n",
    "\n",
    "    print (\"searching through hashed list...\")\n",
    "    tt = time()\n",
    "    result = searchThroughHash(string)\n",
    "    print (\"Best match: \" + result)\n",
    "    print ('searching through hashed list took ' + str(time()-tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#result = searchThroughList(\"pillow\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_string_matching(pillow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first pass produces many matches that it shouldn't and also matches a single firm name to multiple other names. I need to clean it up so that I have (1) each firm is associated with only one group of firm names and (2) for the survey, I'm only looking at firms that are matched to others. (I will go back to the original pairings code when creating the manual crosswalk.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Don't match firm names to themselves\n",
    "noself = []\n",
    "for j in range(len(pairings)):\n",
    "    noself.append([pairings[j][0],[i for i in pairings[j][1] if pairings[j][0] != i]])\n",
    "\n",
    "# For the survey, only get those firm names that were matched with at least one other\n",
    "survey1 = [noself[i] for i in range(len(noself)) if len(noself[i][1]) > 0]\n",
    "print(survey1[0])\n",
    "\n",
    "# However, this produces many clearly false matches. The token_set_ratio function is too greedy so I apply a different function to the paired data.\n",
    "lessgreedy = []\n",
    "for j in range(len(survey1)):\n",
    "    lessgreedy.append([survey1[j][0],[x[0] for x in process.extractBests(survey1[j][0],survey1[j][1],scorer = fuzz.token_sort_ratio,score_cutoff=70)]])\n",
    "print(lessgreedy[0])\n",
    "\n",
    "# Again, for the survey only use matches that were paired with at least one other\n",
    "survey2 = [lessgreedy[i] for i in range(len(lessgreedy)) if len(lessgreedy[i][1]) > 0]\n",
    "\n",
    "# Finally, for the survey I want to remove all instances in which the 'anchor' and the 'pair(s)' simply flip themselves.\n",
    "forsurvey=[]\n",
    "for i in range(len(survey2)-1):\n",
    "    if len(survey2[i][1]) == 1:\n",
    "        j = i+1\n",
    "        if [survey2[i][0],survey2[i][1][0]] != [survey2[j][1][0],survey2[j][0]]:\n",
    "            forsurvey.append(survey2[i])\n",
    "    else:\n",
    "        forsurvey.append(survey2[i])\n",
    "forsurvey[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we still find that an anchor's pair may itself have other pairs that aren't just the inverse. For example, \\#1 CLEANERS is paired with \\$1.25 CLEANERS but this name is also associated with \\$1.25 DRY-CLEANERS. Ideally, I want to take all names that are associated with multiple pairs and run a sophisticated algorithm that will identify the best possible pairing (i.e., \\$1.25 DRY-CLEANERS, not \\#1 CLEANERS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We know that anchors can only show up once in the full list of anchors.\n",
    "anchors = [forsurvey[x][0] for x in range(len(forsurvey))]\n",
    "anchors[:10]\n",
    "\n",
    "# We are interested in whether a paired name shows up elsewhere as (1) its own anchor (i.e., after having stripped inverses)\n",
    "# or (2) as a pair to a different anchor.\n",
    "pairs=[]\n",
    "for j in range(len(forsurvey)):\n",
    "    for i in range(len(forsurvey[j][1])):\n",
    "        pairs.append(forsurvey[j][1][i])\n",
    "pairs[:10]\n",
    "\n",
    "# For the first question, if a pair shows up as its own anchor, this is the same as an anchor showing up in the pairs\n",
    "anchordupes = []\n",
    "for j in range(len(forsurvey)):\n",
    "    temp = []\n",
    "    for i in range(len(forsurvey)):\n",
    "        if forsurvey[j][0] in forsurvey[i][1]:\n",
    "            for k in range(len(forsurvey[i][1])):\n",
    "                if forsurvey[j][0] != forsurvey[i][1][k]:\n",
    "                    temp.append(forsurvey[i][1][k])\n",
    "    anchordupes.append([forsurvey[j][0],list(set(temp))])\n",
    "        \n",
    "# We now clean the anchor duplicates and choose the best possible match for those that remain\n",
    "anchordupes1 = [anchordupes[i] for i in range(len(anchordupes)) if len(anchordupes[i][1]) > 0]\n",
    "\n",
    "# For the second question, we simple repeat the above but go through all pairs that aren't the self\n",
    "forsurvey[0]\n",
    "pairs = list(set(pairs))\n",
    "\n",
    "pairsdupes = []\n",
    "for j in range(len(pairs)):\n",
    "    temp=[]\n",
    "    for i in range(len(forsurvey)):\n",
    "        if pairs[j] in forsurvey[i][1]:\n",
    "            temp.append(forsurvey[i][0])\n",
    "    pairsdupes.append([pairs[j],temp])\n",
    "\n",
    "pairsdupes1 = [pairsdupes[i] for i in range(len(pairsdupes)) if len(pairsdupes[i][1]) > 0]\n",
    "pairsdupes1\n",
    "\n",
    "# Finally, I merge these together by combining all matches for each unique anchor\n",
    "fullanchors = [anchordupes1[x][0] for x in range(len(anchordupes1))]\n",
    "fullanchors = list(set(fullanchors))\n",
    "fullanchors\n",
    "\n",
    "fullpairs = [pairsdupes1[x][0] for x in range(len(pairsdupes1))]\n",
    "fullpairs = list(set(fullpairs))\n",
    "fullpairs\n",
    "\n",
    "full = fullanchors + fullpairs\n",
    "full = list(set(full))\n",
    "full\n",
    "\n",
    "pulls = []\n",
    "for i in range(len(full)):\n",
    "    temp1=[]\n",
    "    for j in range(len(anchordupes1)):\n",
    "        if full[i] == anchordupes1[j][0]:\n",
    "            temp1.append(anchordupes1[j][1])\n",
    "    temp2 = []\n",
    "    for j in range(len(pairsdupes1)):\n",
    "        if full[i] == pairsdupes1[j][0]:\n",
    "            temp2.append(pairsdupes1[j][1])\n",
    "    if len(temp1) > 0 or len(temp2) > 0:\n",
    "        all = []\n",
    "        if len(temp1) > 0:\n",
    "            for x in range(len(temp1)):\n",
    "                all.append(temp1[x])\n",
    "        if len(temp2) > 0:\n",
    "            for y in range(len(temp2)):\n",
    "                all.append(temp2[y])\n",
    "        all2 = []\n",
    "        for z in range(len(all)):\n",
    "            for a in range(len(all[z])):\n",
    "                all2.append(all[z][a])\n",
    "    pulls.append([full[i],list(set(all2))])\n",
    "\n",
    "\n",
    "# Jesus...almost there. I think the last step is very similar to the first one. I need to aggregate all daisy-chained \n",
    "# pairs into a single grouping.\n",
    "finaltemp = []\n",
    "for i in range(len(pulls)):\n",
    "    temp=[]\n",
    "    if len(pulls[i][1]) > 0:\n",
    "        for j in range(len(pulls[i][1])):\n",
    "            for k in range(len(pulls)):\n",
    "                if pulls[i][1][j] == pulls[k][0] and i != k:\n",
    "                    #print('pair %s in pull %s is at %s' %(j,i,k))\n",
    "                    for l in range(len(pulls[k][1])):\n",
    "                        if pulls[k][1][l] != pulls[i][0]:\n",
    "                            temp.append(pulls[k][1][l])\n",
    "                    if k > i:\n",
    "                        pulls[k][0] = \"DUPES\"\n",
    "        finaltemp.append([pulls[i][0],list(set(temp))])\n",
    "\n",
    "final = []\n",
    "for i in range(len(finaltemp)):\n",
    "    if finaltemp[i][0] != 'DUPES' and len(finaltemp[i][1])>0:\n",
    "        final.append(finaltemp[i])\n",
    "\n",
    "final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jesus. If Drew every saw that mess, he would kill me. I just used 200 lines of code to accomplish something that I reckon should be pretty simple. But fuck it. I am building intuition for pyton. \n",
    "\n",
    "And what exactly did I accomplish? I'm not entirely sure. I've created a list of firm names that aggregates all the possible pairings that have exceeded the two thresholds I've put them through via the fuzzywuzzy tools. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write a .txt file in the format for importing to Qualtrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below can be converted to illustrate the process by which I create the .txt files that are formatted to be imported into Qualtrics. I don't run the code here though because it would take a long time."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "submask = mask[:20]\n",
    "counter = 0\n",
    "for j in range(0,len(submask),10):\n",
    "    print(\"Block Test: %s\" % (j / 10))\n",
    "    for k in range(j,j+10):\n",
    "        print(\"     Advanced Choices: %s\" % submask[k][0] + \" %s\" % counter)\n",
    "        for i in range(0,len(submask[k][1])):\n",
    "            print(\"          Choice: %s\" % i + \" %s\" % submask[k][1][i])\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxs = int(math.ceil(len(final) / 100))\n",
    "print(maxs)\n",
    "length = len(final)\n",
    "\n",
    "start = 0\n",
    "counter = 0\n",
    "\n",
    "for z in range(1,maxs+1):\n",
    "    end = min(z*100,length)\n",
    "    submask = final[start:end]\n",
    "    start = end+1\n",
    "    survey_file = open(\"range%s_survey_test_%s\" %(1,z) +\".txt\",\"w\")\n",
    "    survey_file.write(\"[[AdvancedFormat]] \\n\\n\")\n",
    "    for j in range(0,len(submask),10):\n",
    "        survey_file.write(\"[[Block: Firm Names - %s\" % counter + \"]]\\n\")\n",
    "        for k in range(j,min(j+10,len(submask))):\n",
    "            survey_file.write(\" [[Question:MC:MultipleAnswer:Vertical]] \\n \\\n",
    "            %s\" % submask[k][0].upper() + \"\\n\\n\")\n",
    "            survey_file.write(\"  [[AdvancedChoices]] \\n\")\n",
    "            for i in range(0,len(submask[k][1])):\n",
    "                survey_file.write(\"    [[Choice: %s\" % i +\"]] \\n \\\n",
    "                %s\" % submask[k][1][i].upper() +'\\n')\n",
    "            survey_file.write(\"\\n\")\n",
    "        progbar(counter,maxs*10)\n",
    "        counter += 1\n",
    "    survey_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for my final act, I will assign unique ids to every firm name in the dataset. This means I need to reincorporate the individual values assembled above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indivs = [pairings[x][0] for x in range(len(pairings))]\n",
    "finalanchors = [final[x][0] for x in range(len(final))]\n",
    "all = []\n",
    "for i in range(len(indivs)):\n",
    "    if indivs[i] in finalanchors:\n",
    "        for j in range(len(finalanchors)):\n",
    "            if indivs[i] == final[j][0]:\n",
    "                all.append(final[j])\n",
    "    else:\n",
    "        all.append([indivs[i],[]])\n",
    "all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crosswalk1 = []\n",
    "bisids=[]\n",
    "for i in range(len(all)):\n",
    "    crosswalk1.append(all[i][0])\n",
    "    bisids.append(\"bid\"+str(i))\n",
    "    for j in range(len(all[i][1])):\n",
    "        crosswalk1.append(all[i][1][j])\n",
    "        bisids.append(\"bid\"+str(i))\n",
    "\n",
    "crosswalk = pd.DataFrame({'cw1' : crosswalk1,'bid':bisids})\n",
    "print(forsurvey[0])\n",
    "print(forsurvey[0][1])\n",
    "#print(mask[23][1][0][0])\n",
    "\n",
    "#inds = []\n",
    "#for x in range(len(forsurvey)):\n",
    "#    for i in range(len(forsurvey[x][1])):\n",
    "#         if mask[23][1][0] in forsurvey[x][1][i]:\n",
    "#                inds.append(x)\n",
    "#mask[inds[1]]\n",
    "#inds\n",
    "\n",
    "crosswalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args = 1\n",
    "name = str('test%s' % args + '.csv')\n",
    "header = ['bid','cw1']\n",
    "crosswalk.to_csv(name,sep=',',columns=header,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "git add fec-string-prep.ipynb\n",
    "git status\n",
    "git commit -m \"Further tinkering with the algorithm and attempting to get things parallelized.\"\n",
    "git push\n",
    "git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
